{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(probs, labels):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss.\n",
    "    Assumes labels are class indices.\n",
    "    \"\"\"\n",
    "    N = probs.shape[0]\n",
    "    loss = -np.sum(np.log(probs[np.arange(N), labels])) / N\n",
    "    return loss\n",
    "\n",
    "def d_loss_softmax(probs, labels):\n",
    "    \"\"\"\n",
    "    Computes gradient of loss with respect to logits,\n",
    "    assuming a softmax output and cross-entropy loss.\n",
    "    \"\"\"\n",
    "    N = probs.shape[0]\n",
    "    grad = probs.copy()\n",
    "    grad[np.arange(N), labels] -= 1\n",
    "    grad /= N\n",
    "    return grad\n",
    "\n",
    "# ---------------------------\n",
    "# Convolutional Layer Class\n",
    "# ---------------------------\n",
    "class Conv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, learning_rate=0.01, stride=1, padding=0, activation='relu'):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate    \n",
    "        \n",
    "        # Xavier initialization\n",
    "        fan_in = in_channels * kernel_size * kernel_size\n",
    "        fan_out = out_channels * kernel_size * kernel_size\n",
    "        limit = np.sqrt(2 / (fan_in + fan_out))\n",
    "        self.weights = np.random.uniform(-limit, limit, (out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.biases = np.zeros((out_channels, 1))\n",
    "        \n",
    "    def pad_input(self, x):\n",
    "        if self.padding > 0:\n",
    "            return np.pad(x, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: performs convolution and applies ReLU.\n",
    "        Stores the pre-activation output for backpropagation.\n",
    "        \"\"\"\n",
    "        self.input = x\n",
    "        x_padded = self.pad_input(x)\n",
    "        batch_size, _, input_height, input_width = x.shape\n",
    "        \n",
    "        out_height = (input_height - self.kernel_size + 2*self.padding) // self.stride + 1\n",
    "        out_width  = (input_width  - self.kernel_size + 2*self.padding) // self.stride + 1\n",
    "        \n",
    "        conv_pre = np.zeros((batch_size, self.out_channels, out_height, out_width))\n",
    "        # Convolution operation\n",
    "        for b in range(batch_size):\n",
    "            for oc in range(self.out_channels):\n",
    "                for i in range(out_height):\n",
    "                    for j in range(out_width):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "                        conv_pre[b, oc, i, j] = np.sum(\n",
    "                            x_padded[b, :, h_start:h_end, w_start:w_end] * self.weights[oc]\n",
    "                        ) + self.biases[oc]\n",
    "                        \n",
    "        self.conv_pre = conv_pre  # store pre-activation values\n",
    "        # Apply ReLU activation\n",
    "        self.output = np.maximum(0, conv_pre)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Backward pass: computes gradients with respect to the input,\n",
    "        weights, and biases. It assumes that the gradient d_out is already\n",
    "        with respect to the activated output.\n",
    "        \"\"\"\n",
    "        batch_size, _, out_height, out_width = d_out.shape\n",
    "        x_padded = self.pad_input(self.input)\n",
    "        d_x = np.zeros_like(x_padded)\n",
    "        d_w = np.zeros_like(self.weights)\n",
    "        d_b = np.zeros_like(self.biases)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for oc in range(self.out_channels):\n",
    "                for i in range(out_height):\n",
    "                    for j in range(out_width):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "                        \n",
    "                        d_w[oc] += x_padded[b, :, h_start:h_end, w_start:w_end] * d_out[b, oc, i, j]\n",
    "                        d_x[b, :, h_start:h_end, w_start:w_end] += self.weights[oc] * d_out[b, oc, i, j]\n",
    "                        d_b[oc] += d_out[b, oc, i, j]\n",
    "                        \n",
    "        if self.padding > 0:\n",
    "            d_x = d_x[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "        \n",
    "        return d_x, d_w, d_b\n",
    "\n",
    "    def update_params(self, d_w, d_b):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases using gradient descent.\n",
    "        \"\"\"\n",
    "        self.weights -= self.learning_rate * d_w\n",
    "        self.biases -= self.learning_rate * d_b\n",
    "\n",
    "# ---------------------------\n",
    "# Fully Connected Layer Class\n",
    "# ---------------------------\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_dim, output_dim, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        limit = np.sqrt(6 / (input_dim + output_dim))\n",
    "        self.weights = np.random.uniform(-limit, limit, (input_dim, output_dim))\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass: computes logits.\n",
    "        \"\"\"\n",
    "        self.input = X\n",
    "        self.output = np.dot(X, self.weights) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Backward pass: computes gradients with respect to inputs, weights, and biases.\n",
    "        \"\"\"\n",
    "        self.dweights = np.dot(self.input.T, d_out)\n",
    "        self.dbias = np.sum(d_out, axis=0, keepdims=True)\n",
    "        dx = np.dot(d_out, self.weights.T)\n",
    "        return dx\n",
    "    \n",
    "    def update_params(self):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases using gradient descent.\n",
    "        \"\"\"\n",
    "        self.weights -= self.learning_rate * self.dweights\n",
    "        self.bias -= self.learning_rate * self.dbias\n",
    "\n",
    "# ---------------------------\n",
    "# Sample Training Step\n",
    "# ---------------------------\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Dummy input: 2 samples, 1 channel, 5x5 image\n",
    "    X = np.random.randn(2, 1, 5, 5)\n",
    "    # Dummy labels: 2 samples with class indices (assume 3 classes: 0, 1, 2)\n",
    "    labels = np.array([0, 2])\n",
    "    \n",
    "    # Initialize layers\n",
    "    conv = Conv2D(in_channels=1, out_channels=2, kernel_size=3, learning_rate=0.01, stride=1, padding=0, activation='relu')\n",
    "    # After conv: output shape will be (2, 2, 3, 3). Flatten each sample to 2*3*3 = 18 features.\n",
    "    fc = FullyConnectedLayer(input_dim=2*3*3, output_dim=3, learning_rate=0.01)\n",
    "    \n",
    "    # -------- Forward Pass --------\n",
    "    conv_out = conv.forward(X)                # Conv layer forward (with ReLU applied)\n",
    "    fc_input = conv_out.reshape(conv_out.shape[0], -1)  # Flatten conv output for FC layer\n",
    "    fc_logits = fc.forward(fc_input)            # FC layer forward (logits)\n",
    "    probs = softmax(fc_logits)                  # Softmax activation for classification\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = cross_entropy_loss(probs, labels)\n",
    "    print(\"Loss:\", loss)\n",
    "    \n",
    "    # -------- Backward Pass --------\n",
    "    # Compute gradient of loss w.r.t. logits (softmax + cross-entropy derivative)\n",
    "    d_logits = d_loss_softmax(probs, labels)    # shape: (2, 3)\n",
    "    \n",
    "    # Backprop through Fully Connected Layer\n",
    "    d_fc_input = fc.backward(d_logits)          # shape: (2, 18)\n",
    "    fc.update_params()                          # Update FC layer parameters\n",
    "    \n",
    "    # Reshape gradient to match conv layer output shape: (2, 2, 3, 3)\n",
    "    d_conv_out = d_fc_input.reshape(conv_out.shape)\n",
    "    \n",
    "    # --- Backprop through Conv Layer ---\n",
    "    # Since the conv layer forward applied ReLU, we must compute its derivative externally.\n",
    "    # The derivative of ReLU is 1 where conv_pre > 0 and 0 otherwise.\n",
    "    d_conv_pre = d_conv_out * (conv.conv_pre > 0)\n",
    "    \n",
    "    # Backprop through convolution layer\n",
    "    d_X, d_w, d_b = conv.backward(d_conv_pre)\n",
    "    conv.update_params(d_w, d_b)                # Update Conv layer parameters\n",
    "    \n",
    "    print(\"Backward pass complete. Parameters updated.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
