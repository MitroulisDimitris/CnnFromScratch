{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output:\n",
      " [[-0.04441203  0.78826677  0.57660598]\n",
      " [ 0.2661469   0.39007234  0.45195581]\n",
      " [-0.11615365 -0.88345875  0.39322788]\n",
      " [ 0.11939388  1.0420691   0.06827389]\n",
      " [ 0.16113172  0.71970105  0.8418569 ]]\n",
      "\n",
      "Gradient with respect to input (dx):\n",
      " [[ 0.41979034  0.04064291  1.07199642  0.46623325]\n",
      " [ 0.26547981  0.08906956  0.64023037  0.1774453 ]\n",
      " [-0.28769188  0.2214638  -0.28985703 -0.43111009]\n",
      " [ 0.43899848 -0.11894462  0.80075827  0.54077901]\n",
      " [ 0.46152035  0.13907364  1.22623105  0.39902649]]\n",
      "\n",
      "Analytical gradient for weights (dweights):\n",
      " [[ 1.17835853e+00  6.90654483e+00  1.55551558e+00]\n",
      " [-7.28083142e-02 -1.01960974e+00  1.37215174e+00]\n",
      " [-6.36805741e-03 -1.32492681e-01  8.70915047e-01]\n",
      " [-1.17343536e-01  6.62642904e-01 -1.79875942e-01]]\n",
      "\n",
      "Analytical gradient for bias (dbias):\n",
      " [[0.38610682 2.0566505  2.33192046]]\n",
      "\n",
      "Numerical gradient for weights (num_dweights):\n",
      " [[ 1.17835853e+00  6.90654483e+00  1.55551558e+00]\n",
      " [-7.28083142e-02 -1.01960974e+00  1.37215174e+00]\n",
      " [-6.36805741e-03 -1.32492681e-01  8.70915047e-01]\n",
      " [-1.17343536e-01  6.62642904e-01 -1.79875942e-01]]\n",
      "\n",
      "Max absolute difference in weights gradient: 3.2253338888565963e-11\n",
      "\n",
      "Numerical gradient for bias (num_dbias):\n",
      " [[0.38610682 2.0566505  2.33192046]]\n",
      "\n",
      "Max absolute difference in bias gradient: 2.6795898833142928e-11\n"
     ]
    }
   ],
   "source": [
    "from fully_connected import FullyConnectedLayer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test_fully_connected_layer():\n",
    "    # Set up test parameters\n",
    "    np.random.seed(0)\n",
    "    input_dim = 4\n",
    "    output_dim = 3\n",
    "    batch_size = 5\n",
    "    \n",
    "    # Create a FullyConnectedLayer instance\n",
    "    fc = FullyConnectedLayer(input_dim, output_dim, learning_rate=0.01)\n",
    "    \n",
    "    # Create random input data\n",
    "    X = np.random.randn(batch_size, input_dim)\n",
    "    \n",
    "    # -------- Forward Pass --------\n",
    "    output = fc.forward(X)\n",
    "    print(\"Forward pass output:\\n\", output)\n",
    "    \n",
    "    # -------- Compute Loss & d_out --------\n",
    "    # Using squared loss: L = 0.5 * sum(output^2)\n",
    "    loss = 0.5 * np.sum(output ** 2)\n",
    "    # The derivative of this loss with respect to output is simply output\n",
    "    d_out = output\n",
    "    \n",
    "    # -------- Backward Pass --------\n",
    "    dx,_,_ = fc.backward(d_out)\n",
    "    print(\"\\nGradient with respect to input (dx):\\n\", dx)\n",
    "    print(\"\\nAnalytical gradient for weights (dweights):\\n\", fc.dweights)\n",
    "    print(\"\\nAnalytical gradient for bias (dbias):\\n\", fc.dbias)\n",
    "    \n",
    "    # -------- Gradient Check --------\n",
    "    epsilon = 1e-5\n",
    "    # Numerical gradient for weights\n",
    "    num_dweights = np.zeros_like(fc.weights)\n",
    "    for i in range(fc.weights.shape[0]):\n",
    "        for j in range(fc.weights.shape[1]):\n",
    "            original_val = fc.weights[i, j]\n",
    "            # Perturb weights positively\n",
    "            fc.weights[i, j] = original_val + epsilon\n",
    "            output_plus = fc.forward(X)\n",
    "            loss_plus = 0.5 * np.sum(output_plus ** 2)\n",
    "            \n",
    "            # Perturb weights negatively\n",
    "            fc.weights[i, j] = original_val - epsilon\n",
    "            output_minus = fc.forward(X)\n",
    "            loss_minus = 0.5 * np.sum(output_minus ** 2)\n",
    "            \n",
    "            # Restore original value\n",
    "            fc.weights[i, j] = original_val\n",
    "            \n",
    "            # Compute numerical gradient\n",
    "            num_dweights[i, j] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "    \n",
    "    # Numerical gradient for bias\n",
    "    num_dbias = np.zeros_like(fc.bias)\n",
    "    for j in range(fc.bias.shape[1]):\n",
    "        original_val = fc.bias[0, j]\n",
    "        fc.bias[0, j] = original_val + epsilon\n",
    "        output_plus = fc.forward(X)\n",
    "        loss_plus = 0.5 * np.sum(output_plus ** 2)\n",
    "        \n",
    "        fc.bias[0, j] = original_val - epsilon\n",
    "        output_minus = fc.forward(X)\n",
    "        loss_minus = 0.5 * np.sum(output_minus ** 2)\n",
    "        \n",
    "        fc.bias[0, j] = original_val\n",
    "        num_dbias[0, j] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "    \n",
    "    print(\"\\nNumerical gradient for weights (num_dweights):\\n\", num_dweights)\n",
    "    print(\"\\nMax absolute difference in weights gradient:\", np.max(np.abs(fc.dweights - num_dweights)))\n",
    "    \n",
    "    print(\"\\nNumerical gradient for bias (num_dbias):\\n\", num_dbias)\n",
    "    print(\"\\nMax absolute difference in bias gradient:\", np.max(np.abs(fc.dbias - num_dbias)))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    test_fully_connected_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference in weight gradient: 1.6426970894656279e-10\n",
      "Max absolute difference in bias gradient: 9.207967721636123e-11\n",
      "Backward pass gradient check PASSED!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_backward():\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "\n",
    "    # Define layer dimensions\n",
    "    batch_size = 5\n",
    "    input_dim = 4\n",
    "    output_dim = 3\n",
    "\n",
    "    # Create random inputs and weights\n",
    "    inputs = np.random.randn(batch_size, input_dim)\n",
    "    weights = np.random.randn(input_dim, output_dim)\n",
    "    bias = np.random.randn(1, output_dim)\n",
    "    \n",
    "    # Create a fake upstream gradient (dout) from next layer\n",
    "    dout = np.random.randn(batch_size, output_dim)\n",
    "\n",
    "    # Define FullyConnectedLayer class\n",
    "    class FullyConnectedLayer:\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            self.input_dim = input_dim\n",
    "            self.output_dim = output_dim\n",
    "            self.weights = np.random.randn(input_dim, output_dim)\n",
    "            self.bias = np.random.randn(1, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            self.input = x\n",
    "            return np.dot(x, self.weights) + self.bias\n",
    "        \n",
    "        def backward(self, dout):\n",
    "            self.dweights = np.dot(self.input.T, dout)\n",
    "            self.dbias = np.sum(dout, axis=0, keepdims=True)\n",
    "            dx = np.dot(dout, self.weights.T)\n",
    "            return dx, self.dweights, self.dbias\n",
    "    \n",
    "    fc = FullyConnectedLayer(input_dim, output_dim)\n",
    "    fc.input = inputs\n",
    "    fc.weights = weights\n",
    "    fc.bias = bias\n",
    "\n",
    "    # Compute gradients using backward pass\n",
    "    dx, dweights, dbias = fc.backward(dout)\n",
    "\n",
    "    # Numerical gradient checking\n",
    "    epsilon = 1e-5\n",
    "    num_dweights = np.zeros_like(weights)\n",
    "    num_dbias = np.zeros_like(bias)\n",
    "\n",
    "    # Compute numerical gradients for weights\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            weights[i, j] += epsilon\n",
    "            loss1 = np.sum((np.dot(inputs, weights) + bias) * dout)  # Forward with perturbed weight\n",
    "            weights[i, j] -= 2 * epsilon\n",
    "            loss2 = np.sum((np.dot(inputs, weights) + bias) * dout)  # Forward with perturbed weight\n",
    "            weights[i, j] += epsilon  # Reset to original\n",
    "            num_dweights[i, j] = (loss1 - loss2) / (2 * epsilon)\n",
    "\n",
    "    # Compute numerical gradients for bias\n",
    "    for j in range(bias.shape[1]):\n",
    "        bias[0, j] += epsilon\n",
    "        loss1 = np.sum((np.dot(inputs, weights) + bias) * dout)\n",
    "        bias[0, j] -= 2 * epsilon\n",
    "        loss2 = np.sum((np.dot(inputs, weights) + bias) * dout)\n",
    "        bias[0, j] += epsilon  # Reset to original\n",
    "        num_dbias[0, j] = (loss1 - loss2) / (2 * epsilon)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Max absolute difference in weight gradient:\", np.max(np.abs(dweights - num_dweights)))\n",
    "    print(\"Max absolute difference in bias gradient:\", np.max(np.abs(dbias - num_dbias)))\n",
    "\n",
    "    # Check if gradients are close\n",
    "    assert np.allclose(dweights, num_dweights, atol=1e-4), \"Weight gradients do not match numerical gradients!\"\n",
    "    assert np.allclose(dbias, num_dbias, atol=1e-4), \"Bias gradients do not match numerical gradients!\"\n",
    "    print(\"Backward pass gradient check PASSED!\")\n",
    "\n",
    "# Run test\n",
    "test_backward()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
