{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass Test:\n",
      "Input:\n",
      " [[[[ 0.  1.  2.  3.]\n",
      "   [ 4.  5.  6.  7.]\n",
      "   [ 8.  9. 10. 11.]\n",
      "   [12. 13. 14. 15.]]]]\n",
      "Expected Output:\n",
      " [[[[ 5.  7.]\n",
      "   [13. 15.]]]]\n",
      "MaxPool Forward Output:\n",
      " [[[[ 5.  7.]\n",
      "   [13. 15.]]]]\n",
      "Forward pass test passed.\n",
      "\n",
      "Backward Pass Test (Analytical Gradients):\n",
      "Input:\n",
      " [[[[ 0.  1.  2.  3.]\n",
      "   [ 4.  5.  6.  7.]\n",
      "   [ 8.  9. 10. 11.]\n",
      "   [12. 13. 14. 15.]]]]\n",
      "Upstream Gradient d_out:\n",
      " [[[[1. 1.]\n",
      "   [1. 1.]]]]\n",
      "Expected d_x:\n",
      " [[[[0. 0. 0. 0.]\n",
      "   [0. 1. 0. 1.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 1. 0. 1.]]]]\n",
      "Computed d_x:\n",
      " [[[[0. 0. 0. 0.]\n",
      "   [0. 1. 0. 1.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 1. 0. 1.]]]]\n",
      "Backward pass test (analytical) passed.\n",
      "\n",
      "Backward Pass Numerical Gradient Check:\n",
      "Analytical d_x:\n",
      " [[[[1. 0. 0. 0.]\n",
      "   [0. 0. 1. 0.]\n",
      "   [0. 1. 1. 0.]\n",
      "   [0. 0. 0. 0.]]]]\n",
      "Numerical d_x:\n",
      " [[[[1. 0. 0. 0.]\n",
      "   [0. 0. 1. 0.]\n",
      "   [0. 1. 1. 0.]\n",
      "   [0. 0. 0. 0.]]]]\n",
      "Numerical gradient check passed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from maxpooling import MaxPool\n",
    "import numpy as np\n",
    "\n",
    "def test_maxpool_forward():\n",
    "    # Use a fixed input with distinct values for clarity.\n",
    "    x = np.array([[[[0, 1, 2, 3],\n",
    "                    [4, 5, 6, 7],\n",
    "                    [8, 9, 10, 11],\n",
    "                    [12, 13, 14, 15]]]], dtype=np.float32)\n",
    "    # Expected output for kernel_size=2, stride=2, padding=0:\n",
    "    # For each 2x2 window, the max values are:\n",
    "    # Window 1: [[0,1],[4,5]] -> max is 5\n",
    "    # Window 2: [[2,3],[6,7]] -> max is 7\n",
    "    # Window 3: [[8,9],[12,13]] -> max is 13\n",
    "    # Window 4: [[10,11],[14,15]] -> max is 15\n",
    "    expected_out = np.array([[[[5, 7],\n",
    "                               [13, 15]]]], dtype=np.float32)\n",
    "\n",
    "    pool = MaxPool(kernel_size=2, stride=2, padding=0)\n",
    "    out = pool.forward(x)\n",
    "    \n",
    "    print(\"Forward Pass Test:\")\n",
    "    print(\"Input:\\n\", x)\n",
    "    print(\"Expected Output:\\n\", expected_out)\n",
    "    print(\"MaxPool Forward Output:\\n\", out)\n",
    "    \n",
    "    assert np.allclose(out, expected_out), \"Forward pass output does not match expected values.\"\n",
    "    print(\"Forward pass test passed.\\n\")\n",
    "\n",
    "def test_maxpool_backward():\n",
    "    # Use the same fixed input.\n",
    "    x = np.array([[[[0, 1, 2, 3],\n",
    "                    [4, 5, 6, 7],\n",
    "                    [8, 9, 10, 11],\n",
    "                    [12, 13, 14, 15]]]], dtype=np.float32)\n",
    "    pool = MaxPool(kernel_size=2, stride=2, padding=0)\n",
    "    out = pool.forward(x)\n",
    "    \n",
    "    # Assume the loss is simply the sum of the outputs,\n",
    "    # so the upstream gradient is an array of ones.\n",
    "    d_out = np.ones_like(out)\n",
    "    d_x = pool.backward(d_out)\n",
    "    \n",
    "    # For max pooling, the gradient with respect to the input is 1 at the location\n",
    "    # of the max in each pooling window, and 0 elsewhere.\n",
    "    expected_d_x = np.zeros_like(x)\n",
    "    # Window 1 (top-left 2x2): max at position (1,1)\n",
    "    expected_d_x[0, 0, 1, 1] = 1.\n",
    "    # Window 2 (top-right 2x2): max at position (1,3)\n",
    "    expected_d_x[0, 0, 1, 3] = 1.\n",
    "    # Window 3 (bottom-left 2x2): max at position (3,1)\n",
    "    expected_d_x[0, 0, 3, 1] = 1.\n",
    "    # Window 4 (bottom-right 2x2): max at position (3,3)\n",
    "    expected_d_x[0, 0, 3, 3] = 1.\n",
    "    \n",
    "    print(\"Backward Pass Test (Analytical Gradients):\")\n",
    "    print(\"Input:\\n\", x)\n",
    "    print(\"Upstream Gradient d_out:\\n\", d_out)\n",
    "    print(\"Expected d_x:\\n\", expected_d_x)\n",
    "    print(\"Computed d_x:\\n\", d_x)\n",
    "    \n",
    "    assert np.allclose(d_x, expected_d_x), \"Backward pass gradients do not match expected values.\"\n",
    "    print(\"Backward pass test (analytical) passed.\\n\")\n",
    "\n",
    "def test_maxpool_backward_numerical():\n",
    "    \"\"\"\n",
    "    Numerical gradient check for the maxpool layer with respect to its input.\n",
    "    The function being tested is f(x) = sum(maxpool(x)),\n",
    "    so dL/dx is approximated using central differences.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    # Use a small input to avoid boundary issues.\n",
    "    x = np.random.randn(1, 1, 4, 4).astype(np.float64)\n",
    "    pool = MaxPool(kernel_size=2, stride=2, padding=0)\n",
    "    \n",
    "    # Define a function that returns the sum of the maxpool output.\n",
    "    def f(x_input):\n",
    "        # Each call to forward caches variables in the pool object.\n",
    "        return np.sum(pool.forward(x_input))\n",
    "    \n",
    "    # Compute the analytical gradient.\n",
    "    out = pool.forward(x)\n",
    "    d_out = np.ones_like(out)\n",
    "    d_x_analytical = pool.backward(d_out)\n",
    "    \n",
    "    # Numerical gradient: use a small epsilon for central differences.\n",
    "    epsilon = 1e-5\n",
    "    d_x_numerical = np.zeros_like(x)\n",
    "    \n",
    "    # Iterate over every element in x.\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        original_value = x[idx]\n",
    "        \n",
    "        # Compute f(x + epsilon)\n",
    "        x[idx] = original_value + epsilon\n",
    "        f_plus = f(x)\n",
    "        \n",
    "        # Compute f(x - epsilon)\n",
    "        x[idx] = original_value - epsilon\n",
    "        f_minus = f(x)\n",
    "        \n",
    "        # Restore the original value.\n",
    "        x[idx] = original_value\n",
    "        \n",
    "        # Compute the numerical gradient.\n",
    "        d_x_numerical[idx] = (f_plus - f_minus) / (2 * epsilon)\n",
    "        it.iternext()\n",
    "    \n",
    "    print(\"Backward Pass Numerical Gradient Check:\")\n",
    "    print(\"Analytical d_x:\\n\", d_x_analytical)\n",
    "    print(\"Numerical d_x:\\n\", d_x_numerical)\n",
    "    \n",
    "    # Allow a small tolerance, especially since max pooling is non-smooth.\n",
    "    assert np.allclose(d_x_analytical, d_x_numerical, atol=1e-5), \"Numerical gradient check failed.\"\n",
    "    print(\"Numerical gradient check passed.\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_maxpool_forward()\n",
    "    test_maxpool_backward()\n",
    "    test_maxpool_backward_numerical()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
