{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convolution import Conv2D\n",
    "from maxpooling import MaxPool\n",
    "from fully_connected import FullyConnectedLayer\n",
    "from Helper import ReLU, SoftMax, Dropout, Flatten\n",
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,lr=0.01):\n",
    "        self.conv1 = Conv2D(in_channels=1,out_channels=32,kernel_size=3,learning_rate=lr)\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPool(kernel_size=2,stride=2)\n",
    "\n",
    "\n",
    "        self.conv2 = Conv2D(in_channels=32,out_channels=64,kernel_size=3,learning_rate=lr)\n",
    "        self.relu2 = ReLU()\n",
    "        self.pool2 = MaxPool(kernel_size=2,stride=2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.fc1= FullyConnectedLayer(input_dim=5*5*64,output_dim=128,learning_rate=lr)\n",
    "        self.relu3 = ReLU()\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate=0.2)\n",
    "        self.fc2= FullyConnectedLayer(input_dim=128,output_dim=10,learning_rate=lr)\n",
    "\n",
    "        self.softmax = SoftMax()\n",
    "\n",
    "        self.layers = [\n",
    "            self.conv1,\n",
    "            self.pool1,\n",
    "            self.conv2,\n",
    "            self.pool2,\n",
    "            self.flatten,\n",
    "            self.fc1,\n",
    "            self.relu3,\n",
    "            self.dropout,    \n",
    "            self.fc2,\n",
    "            self.softmax\n",
    "        ]\n",
    "             \n",
    "    def forward(self, x, training=True):\n",
    "        \"\"\"\n",
    "        Forward pass through all layers.\n",
    "        :param x: Input batch of shape (batch_size, 1, 28, 28)\n",
    "        :param training: Boolean, True if in training mode (affects dropout)\n",
    "        :return: Predictions after softmax, shape (batch_size, 10)\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,d_out):\n",
    "        \"\"\"\n",
    "        Backward pass through all layers (reverse order).\n",
    "        :param d_out: Gradient of the loss w.r.t. the output of the last layer\n",
    "        :return: Gradient w.r.t. the input of the first layer (not always needed)\n",
    "        \"\"\"\n",
    "        \n",
    "        grad= d_out\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "        \n",
    "    def compute_loss(self, preds, labels):\n",
    "        \"\"\"\n",
    "        Cross-entropy loss.\n",
    "        :param preds: Predictions after softmax, shape (batch_size, 10)\n",
    "        :param labels: One-hot encoded true labels, shape (batch_size, 10)\n",
    "        :return: Scalar loss\n",
    "        \"\"\"\n",
    "        # To avoid numerical instability, clip predictions\n",
    "        eps = 1e-9\n",
    "        preds_clipped = np.clip(preds, eps, 1 - eps)\n",
    "\n",
    "        # Cross-entropy\n",
    "        batch_size = preds.shape[0]\n",
    "        loss = -np.sum(labels * np.log(preds_clipped)) / batch_size\n",
    "        return loss\n",
    "\n",
    "    def train_on_batch(self, x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Single training step on a batch of data:\n",
    "        1) Forward pass\n",
    "        2) Compute loss\n",
    "        3) Backward pass\n",
    "        4) Update parameters\n",
    "        :param x_batch: (batch_size, 1, 28, 28)\n",
    "        :param y_batch: (batch_size, 10) one-hot\n",
    "        :return: Scalar loss\n",
    "        \"\"\"\n",
    "        # 1) Forward pass (training=True enables dropout)\n",
    "        preds = self.forward(x_batch, training=True)\n",
    "        \n",
    "        # 2) Compute loss\n",
    "        loss = self.compute_loss(preds, y_batch)\n",
    "        \n",
    "        # 3) Compute gradient of the loss w.r.t. the final layerâ€™s output\n",
    "\n",
    "        batch_size = y_batch.shape[0]\n",
    "        d_out = (preds - y_batch) / batch_size # for softmax + cross-entropy\n",
    "        \n",
    "        # 4) Backward pass\n",
    "        self.backward(d_out)\n",
    "        \n",
    "        # 5) Update parameters in each layer that has update_weights()\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'update_weights'):\n",
    "                layer.update_weights()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Inference (forward pass without dropout).\n",
    "        :param x: (batch_size, 1, 28, 28)\n",
    "        :return: Class probabilities, shape (batch_size, 10)\n",
    "        \"\"\"\n",
    "        probs = self.forward(x, training=False)\n",
    "        return probs\n",
    "\n",
    "    def evaluate_accuracy(self, x, y_true):\n",
    "        \"\"\"\n",
    "        Compute accuracy on a batch of data.\n",
    "        :param x: (batch_size, 1, 28, 28)\n",
    "        :param y_true: (batch_size, 10) one-hot\n",
    "        :return: accuracy (float)\n",
    "        \"\"\"\n",
    "        probs = self.predict(x)  # shape: (batch_size, 10)\n",
    "        y_pred = np.argmax(probs, axis=1)\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "        accuracy = np.mean(y_pred == y_true_labels)\n",
    "        return accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
