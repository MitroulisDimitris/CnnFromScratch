{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Initialization test passed\n",
      "✓ Forward pass test passed\n",
      "✓ Backward pass test passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from convolution import Conv2D\n",
    "\n",
    "def test_initialization():\n",
    "    conv = Conv2D(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    assert conv.weights.shape == (8, 3, 3, 3), f\"Expected weight shape (8, 3, 3, 3), but got {conv.weights.shape}\"\n",
    "    assert conv.biases.shape == (8, 1), f\"Expected bias shape (8, 1), but got {conv.biases.shape}\"\n",
    "\n",
    "    print(\"✓ Initialization test passed\")\n",
    "\n",
    "def test_forward_pass():\n",
    "    conv = Conv2D(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    x = np.random.randn(2, 3, 32, 32)  # Batch size of 2, 3 channels, 32x32 image\n",
    "    output = conv.forward(x)\n",
    "\n",
    "    expected_shape = (2, 8, 32, 32)  # Output should maintain the same spatial size due to padding=1\n",
    "    assert output.shape == expected_shape, f\"Expected output shape {expected_shape}, but got {output.shape}\"\n",
    "\n",
    "    print(\"✓ Forward pass test passed\")\n",
    "\n",
    "def test_backward_pass():\n",
    "    conv = Conv2D(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    x = np.random.randn(2, 3, 32, 32)\n",
    "    output = conv.forward(x)\n",
    "    \n",
    "    d_out = np.random.randn(*output.shape)  # Simulating gradient from the next layer\n",
    "    d_input = conv.backward(d_out)\n",
    "\n",
    "    assert d_input[0].shape == x.shape, f\"Expected gradient shape {x.shape}, but got {d_input.shape}\"\n",
    "\n",
    "    print(\"✓ Backward pass test passed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_initialization()\n",
    "    test_forward_pass()\n",
    "    test_backward_pass()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Check - Weights Relative Error: 1.1114650280120296e-12\n",
      "Gradient Check - Biases Relative Error: 2.7203882433146153e-09\n",
      "Gradient check passed! ✅\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from convolution import Conv2D\n",
    "\n",
    "def numerical_gradient(conv_layer, x, d_out, epsilon=1e-5):\n",
    "    numerical_dW = np.zeros_like(conv_layer.weights)\n",
    "    numerical_db = np.zeros_like(conv_layer.biases)\n",
    "    \n",
    "    # Compute numerical gradient for weights\n",
    "    for i in range(conv_layer.weights.shape[0]):\n",
    "        for j in range(conv_layer.weights.shape[1]):\n",
    "            for k in range(conv_layer.weights.shape[2]):\n",
    "                for l in range(conv_layer.weights.shape[3]):\n",
    "                    original_value = conv_layer.weights[i, j, k, l]\n",
    "                    \n",
    "                    # Perturb weight positively\n",
    "                    conv_layer.weights[i, j, k, l] = original_value + epsilon\n",
    "                    loss1 = np.sum(conv_layer.forward(x) * d_out)\n",
    "                    \n",
    "                    # Perturb weight negatively\n",
    "                    conv_layer.weights[i, j, k, l] = original_value - epsilon\n",
    "                    loss2 = np.sum(conv_layer.forward(x) * d_out)\n",
    "                    \n",
    "                    # Restore original weight\n",
    "                    conv_layer.weights[i, j, k, l] = original_value\n",
    "                    \n",
    "                    numerical_dW[i, j, k, l] = (loss1 - loss2) / (2 * epsilon)\n",
    "    \n",
    "    # Compute numerical gradient for biases\n",
    "    for i in range(conv_layer.biases.shape[0]):\n",
    "        original_value = conv_layer.biases[i, 0]\n",
    "        \n",
    "        conv_layer.biases[i, 0] = original_value + epsilon\n",
    "        loss1 = np.sum(conv_layer.forward(x) * d_out)\n",
    "        \n",
    "        conv_layer.biases[i, 0] = original_value - epsilon\n",
    "        loss2 = np.sum(conv_layer.forward(x) * d_out)\n",
    "        \n",
    "        conv_layer.biases[i, 0] = original_value\n",
    "        \n",
    "        numerical_db[i, 0] = (loss1 - loss2) / (2 * epsilon)\n",
    "    \n",
    "    return numerical_dW, numerical_db\n",
    "\n",
    "def test_gradient(conv_layer):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Use consistent attribute names\n",
    "    x = np.random.randn(1, conv_layer.in_channels, 5, 5)  # Batch size = 1, Input shape = (5,5)\n",
    "    d_out = np.random.randn(1, conv_layer.out_channels, 3, 3)  # Output shape (assuming kernel=3, stride=1, padding=0)\n",
    "\n",
    "    # Compute numerical gradients\n",
    "    numerical_dW, numerical_db = numerical_gradient(conv_layer, x, d_out)\n",
    "\n",
    "    # Compute analytical gradients without updating weights\n",
    "    conv_layer.forward(x)\n",
    "    _, analytical_dW, analytical_db = conv_layer.backward(d_out, update_weights=False)\n",
    "\n",
    "    # Compute relative error for gradients\n",
    "    weight_error = np.linalg.norm(numerical_dW - analytical_dW) / (np.linalg.norm(numerical_dW) + np.linalg.norm(analytical_dW))\n",
    "    bias_error = np.linalg.norm(numerical_db - analytical_db) / (np.linalg.norm(numerical_db) + np.linalg.norm(analytical_db))\n",
    "\n",
    "    print(f\"Gradient Check - Weights Relative Error: {weight_error}\")\n",
    "    print(f\"Gradient Check - Biases Relative Error: {bias_error}\")\n",
    "\n",
    "    assert weight_error < 1e-4, \"Weight gradients are incorrect!\"\n",
    "    assert bias_error < 1e-4, \"Bias gradients are incorrect!\"\n",
    "    print(\"Gradient check passed! ✅\")\n",
    "\n",
    "# Example usage:\n",
    "conv = Conv2D(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=0)\n",
    "conv.learning_rate = 0.01  # Make sure this is set if needed\n",
    "test_gradient(conv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
